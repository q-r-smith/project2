---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive = T, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Quincy Smith (qrs227)

### Introduction 

The dataset that I chose contains the career statistics for NFL Quarterback Tom Brady. This data includes different metrics that measure his performance over the years as well as his impact on the team's performance. A few of the of the numerical variables include winning percentage, pass yards per game, touchdowns thrown, interceptions thrown, and quarterback rating. The categorical variables are year and team. There are also some binary variables which include Pro Bowl, 1st Team All-Pro, and Super Bowl. These variables determine whether or not Brady went to the Pro Bowl, made 1st Team All-Pro, and won the Super Bowl in that given year. In total, there are 35 different variables that each have 21 obseravations (with the exception of QBR which wasn't recorded until 2006), meaning there are a total if 735 distinct obervations (values) in the dataset.

This dataset was chosen because of my love for sports. In the last project, I analyzed different basketball metrics to make statements about different aspects of how the NBA has been changing from year to year as basketball is my favorite sport. I decided to take on a new challenge and analyze football and the NFL. Football has been my second favorite sport for a while now and I took this opportunity to learn more about it. I chose Tom Brady as he is widely considered the greatest player to pick up the pigskin and many people consider him to have 3 different Hall of Fame worthy careers during his 22 year tenure with the NFL. I hope to find out if whether or not that is true.

```{R}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
TB12 <-read_csv("thomas_edward_patrick_brady_jr.csv")
# if your dataset needs tidying, do so here
TB12 %>% filter(2000 < Year, Year < 2022, Year != 2008) -> TB12
# any other code here
TB12 %>% mutate(Year = as.character(Year)) -> TB12
```

Here we are importing the Tom Brady data and tidying it. Thankfully it was already pretty tidy with a small error in size of the CSV file, so the select function is used get rid of all extraneous rows. Further more, Brady's rookie year and the 2008 year will be removed as he did not complete a full game in either of those years. This will leave us with 19 rows per variable and a total of 665 observations.


### Cluster Analysis

```{R}
library(cluster)
cluster_data <- TB12 %>% select(TD, QBWinPer, Int, Yds, Sk, Att)

cluster_data_2 <- TB12 %>% select(8:23, 25:35)
cluster_data_2

sil_width<-vector() 
for(i in 2:10){  
  kms <- kmeans(cluster_data,centers=i)
  sil <- silhouette(kms$cluster,dist(cluster_data))
  sil_width[i]<-mean(sil[,3])
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)
sil_width_2<-vector() 
for(i in 2:10){  
  kms <- kmeans(cluster_data_2,centers=i)
  sil <- silhouette(kms$cluster,dist(cluster_data_2))
  sil_width_2[i]<-mean(sil[,3])
}
max(sil_width_2, na.rm = T)


cluster_data %>% pam(k=2) -> pam1

library(GGally)
TB12 %>% mutate(cluster=as.factor(pam1$clustering)) %>% 
  ggpairs(columns = c("TD","QBWinPer","Int","Yds","Att"), aes(color=cluster))

plot(pam1, which=2)
kmeans2 <- cluster_data %>% kmeans(2)

TB12 %>% mutate(cluster = kmeans2$cluster) -> TB12
```

Two different clustering groups were made during this clustering exercise. The first one was based solely on the TD, QBWinPer, Yds, Att, and Sk variables as they represent both team success and individual success of Brady and his teams. Using the silhouette width method, we determined that it would be best to use two different clusters. Then, each variable was plotted based on their clustering using ggpairs. The ggpairs plot showed that data in cluster one had less TDs and a higher QBWinPer while cluster two had more Att (Attempts) and more Yds (Yards). At first glance this may seem as IF Brady played less of a role in wins in cluster one, but this difference in cluster performance may be due to different personal changes that had affected Brady's stats. It's worth noting that the biggest difference in cluster metric were the Yds and TDs as Brady consistently threw for more yards and touchdowns in his cluster seasons.

Furthermore, a second clustering group was made using the same clustering method, silhouette width. In this cluster, we included all the numerical variables and binary variables to see if the number of clusters would change. Even when using all the variables, 2 was still the ideal clustering as it had a width of 0.547 with all the variables (compared to a width of 0.56 when only using the aforementioned 5 variables). This leads me to believe while Brady is still an amazing player, he may only have two Hall of Fame worthy careers.
    
    
### Dimensionality Reduction with PCA

```{R}

cluster_data %>% cor() %>% eigen() -> eig1

eig1
X <- cluster_data %>% select(TD, QBWinPer, Int, Yds, Sk, Att) %>% scale

PCAscores <- X %*% eig1$vectors
TB12%>%mutate(PC1=PCAscores[,1], PC2=PCAscores[,2])%>%
  ggplot(aes(PC1,PC2,color = PB))+geom_point()
TB12%>%mutate(PC4=PCAscores[,4], PC6=PCAscores[,6])%>%
  ggplot(aes(PC4,PC6,color = PB))+geom_point()



```

Here a PCA test is performed in an attempt to reduce the dimensions of the dataset. In order to keep the data consistent with the first clustering, the same five variables are chosen to conduct the dimension reduction. The first plots PCA score 1 vs. PCA socre 2 which are the scores refering to TDs and QBWinPer. As the data moves to the right on the graph, there are more touchdowns thrown while as a vertical shift in the data indicates a higher win percentage. The data is also colored by whether or not Brady went to the Pro Bowl in that given year. In the PC1 vs. PC2 graph, the the data is clustered near PC2 = 0, signifying that win percentage stayed fairly consistent despite the change in the how mnay touchdowns where thrown. This could mean that Brady had a smaller impact on the game than perceived as win percentage was resistent to TDs. The PC4 and PC6 graph reduces the dimensions of the data based on Yds and Atts. In this graph, as you move to the right, Brady throws for more yards and as you move up, Brady has more attempts (once again, this graph is colored by Pro Bowl Season). This graph somewhat displays an expected result as the dots that are higher on the graph are generally on the right side, signifying that more attempts mean more yards. There is one season where Brady had a greatly above average amount of attempts while having a below average number of yards and he still impressively made the Pro Bowl in that year. The PCA scores are used to account for only about 0.56 of the variability in the data.

###  Linear Classifier

```{R}
TB12 %>% mutate(PB = as.logical(PB)) -> TB12
log_fit <- glm(PB == TRUE ~ TD + Yds + Int + Att + Sk + QBWinPer + Lng + Cmp + GWD + Rate, data = TB12)
prob_reg <- predict(log_fit, type = "response")

class_diag(prob_reg, TB12$PB, positive = T)

table(truth = TB12$PB, predictions = prob_reg>.5)


```

```{R}
set.seed(322)
k = 10

data <- sample_frac(TB12)  #randomly order rows
folds <- rep(1:k, length.out = nrow(data))  #create folds

diags <- NULL

i = 1
#for (i in 1:k) {
    # create training and test sets
    #train <- data[folds != i, ]
   #$ test <- data[folds == i, ]
    #truth <- test$PB
    
    # train model
    #fit <- glm(PB == TRUE ~ TD + Yds + Int + Att + Sk + QBWinPer + Lng + Cmp + GWD + Rate,
               #data =TB12)
    
    # test model
    #probs <- predict(fit, type = "response")
    
    # get performance metrics for each fold
    #diags <- rbind(diags, class_diag(probs, truth))
#}

# average performance metrics across all folds
#summarize_all(diags, mean)
```

Discussion here

### Non-Parametric Classifier

```{R}
library(caret)
knn_fit <- knn3(PB == TRUE ~ TD + Yds + Int + Att + Sk + QBWinPer + Lng + Cmp + GWD + Rate,
               data =TB12)

prob_knn <- predict(knn_fit, newdata = TB12)[, 2]

class_diag(prob_knn, TB12$PB)
table(truth = TB12$PB, predictions = prob_knn>.5)
```

```{R}
set.seed(322)
k = 10

data <- sample_frac(TB12)  #randomly order rows
folds <- rep(1:k, length.out = nrow(data))  #create folds

diags <- NULL

i = 1
for (i in 1:k) {
    # create training and test sets
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$PB
    
    # train model
    fit <- knn3(PB == TRUE ~ TD + Yds + Int + Att + Sk + QBWinPer + Lng + Cmp + GWD + Rate,
               data =TB12)
    
    # test model
    probs <- predict(fit, newdata = test)[, 2]
    # get performance metrics for each fold
    diags <- rbind(diags, class_diag(probs, truth))
}

# average performance metrics across all folds
summarize_all(diags, mean)
```

Discussion


### Regression/Numeric Prediction

```{R}
y <- TB12$PB
x <- TB12 %>% select(8:23)
```

```{R}
# cross-validation of regression model here
```

Discussion

### Python 

```{R}
library(reticulate)
```

```{python}
# python code here
```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




